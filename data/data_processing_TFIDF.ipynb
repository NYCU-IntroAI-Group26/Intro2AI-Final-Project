{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing process for tweets data without emoji and implentment TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# download the necessary resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# set up the necessary resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def clean_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the spelling\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# remove the stop words and stem the words\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning the text: 100%|██████████| 124498/124498 [00:00<00:00, 388697.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing the text: 100%|██████████| 124498/124498 [00:10<00:00, 11959.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('../dataset/process/tweets_cleaned_without_emoji_emoticons.tsv', sep='\\t')\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "print('Cleaning the text...')\n",
    "tqdm.pandas(desc=\"Cleaning the text\")\n",
    "data['text'] = data['text'].progress_apply(clean_punctuation)\n",
    "\n",
    "print('Preprocessing the text...')\n",
    "tqdm.pandas(desc=\"Preprocessing the text\")\n",
    "data['text'] = data['text'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "data.to_csv('../dataset/process/tweets_cleaned_without_emoji_emoticons.tsv-tfidf.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the vectorizer...\n",
      "Transforming the text...\n",
      "  (0, 4054)\t0.5160875055107653\n",
      "  (0, 8093)\t0.856535864196985\n",
      "  (1, 1305)\t0.638266447424805\n",
      "  (1, 7456)\t0.5406237909824336\n",
      "  (1, 7475)\t0.5480345415350208\n",
      "  (2, 1541)\t0.7829713516878458\n",
      "  (2, 3527)\t0.6220577645493284\n",
      "  (3, 2157)\t0.44256169931983286\n",
      "  (3, 3977)\t0.3867091518300435\n",
      "  (3, 4076)\t0.38239311498915535\n",
      "  (3, 9435)\t0.5414293733927013\n",
      "  (3, 9933)\t0.4639233917604366\n",
      "  (4, 510)\t0.2953743726916661\n",
      "  (4, 1740)\t0.3755421581563312\n",
      "  (4, 3977)\t0.2760191754618713\n",
      "  (4, 5165)\t0.1864127109789944\n",
      "  (4, 7891)\t0.33665618847745293\n",
      "  (4, 9518)\t0.3017758871443727\n",
      "  (4, 9535)\t0.2794208230573945\n",
      "  (4, 9592)\t0.6150639899287629\n",
      "  (5, 1085)\t0.7208531114618266\n",
      "  (5, 5487)\t0.6930878672259408\n",
      "  (6, 3005)\t1.0\n",
      "  (7, 2337)\t0.5927768072139536\n",
      "  (7, 5165)\t0.2679389467748097\n",
      "  :\t:\n",
      "  (124493, 5658)\t0.4300348744252286\n",
      "  (124493, 6289)\t0.24143918888678192\n",
      "  (124494, 1729)\t0.39332480276155096\n",
      "  (124494, 2790)\t0.3723975726405098\n",
      "  (124494, 4253)\t0.4552401365145395\n",
      "  (124494, 5920)\t0.39332480276155096\n",
      "  (124494, 6572)\t0.3019183253708922\n",
      "  (124494, 7028)\t0.38110603182377173\n",
      "  (124494, 9975)\t0.3290458666745838\n",
      "  (124495, 333)\t0.6427379012761791\n",
      "  (124495, 6961)\t0.5388062236406199\n",
      "  (124495, 7419)\t0.5445877740357628\n",
      "  (124496, 1965)\t0.41238407311192526\n",
      "  (124496, 2175)\t0.3744638418384641\n",
      "  (124496, 2985)\t0.29200873270204786\n",
      "  (124496, 6754)\t0.505406030551884\n",
      "  (124496, 8790)\t0.2762297923999407\n",
      "  (124496, 9048)\t0.29297435259876037\n",
      "  (124496, 9370)\t0.43229039107612727\n",
      "  (124497, 3590)\t0.5170019156823147\n",
      "  (124497, 3864)\t0.28212016181975114\n",
      "  (124497, 6551)\t0.46278933795706423\n",
      "  (124497, 6871)\t0.33485202137580383\n",
      "  (124497, 8295)\t0.35542182703196185\n",
      "  (124497, 9412)\t0.4477641240638611\n",
      "(124498, 10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# fit the vectorizer\n",
    "print('Fitting the vectorizer...')\n",
    "vectorizer.fit(data['text'])\n",
    "\n",
    "# transform the text\n",
    "print('Transforming the text...')\n",
    "X = vectorizer.transform(data['text'])\n",
    "\n",
    "# save the data\n",
    "scipy.sparse.save_npz('../dataset/process/tfidf_sparse.npz', X)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
