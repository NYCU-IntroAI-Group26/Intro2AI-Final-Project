{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data clean for adding emoji convert to text back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the necessary resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# set up the necessary resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # clean _\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "\n",
    "    # clean multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # clean numbers  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the spelling\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# remove the stop words and stem the words\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(text):\n",
    "    words = text.split()\n",
    "    term_freq = Counter(words)\n",
    "    return term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning the text: 100%|██████████| 124498/124498 [00:05<00:00, 23416.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing the text: 100%|██████████| 124498/124498 [00:36<00:00, 3389.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('../dataset/process/tweets_convert_cleaned_emoticons_emojis.tsv', sep='\\t')\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "print('Cleaning the text...')\n",
    "tqdm.pandas(desc=\"Cleaning the text\")\n",
    "data['text'] = data['text'].progress_apply(clean_punctuation)\n",
    "\n",
    "# save the data\n",
    "data.to_csv('../dataset/process/tweets_convert_cleaned.tsv', sep='\\t', index=False)\n",
    "\n",
    "print('Preprocessing the text...')\n",
    "tqdm.pandas(desc=\"Preprocessing the text\")\n",
    "data['text'] = data['text'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              hell snowbal\n",
      "1                                           bus rock n roll\n",
      "2                                                game chang\n",
      "3                         yo vine hes fuckifn cute hate kfe\n",
      "4         hate wear cloth like wanna walk around shirt wear\n",
      "                                ...                        \n",
      "124493    sentinel editori fbis comey one middl class fa...\n",
      "124494             perfect pussi clip hudgen zac efron nake\n",
      "124495                                protest rise altright\n",
      "124496    tri convers dad vegetarian pointless infuri th...\n",
      "124497                 stand guy gentleman vice presid penc\n",
      "Name: text, Length: 124498, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_words(text):\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    reduced_words = []\n",
    "    for word in words:\n",
    "        if word_counts[word] > 1:\n",
    "            word_counts[word] -= 1\n",
    "        else:\n",
    "            reduced_words.append(word)\n",
    "    return ' '.join(reduced_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length text is 29\n"
     ]
    }
   ],
   "source": [
    "data['reduced_clean_text'] = data['text'].apply(remove_redundant_words)\n",
    "\n",
    "# calculate the term frequency\n",
    "data['term_frequency'] = data['text'].apply(calculate_term_frequency)\n",
    "\n",
    "# print the maximum length text\n",
    "max_length_text = data['text'].apply(lambda x: len(x.split())).max()\n",
    "print(f\"The maximum length text is {max_length_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "data.to_csv('../dataset/process/tweets_convert_cleaned_reduced.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the vectorizer...\n",
      "Transforming the text...\n",
      "  (0, 3994)\t0.5160875055107653\n",
      "  (0, 8132)\t0.856535864196985\n",
      "  (1, 1248)\t0.638266447424805\n",
      "  (1, 7453)\t0.5406237909824336\n",
      "  (1, 7472)\t0.5480345415350208\n",
      "  (2, 1477)\t0.7829713516878458\n",
      "  (2, 3474)\t0.6220577645493284\n",
      "  (3, 2090)\t0.44278689382597475\n",
      "  (3, 3917)\t0.3869059261478409\n",
      "  (3, 4017)\t0.3825876931210054\n",
      "  (3, 9443)\t0.5417048760413472\n",
      "  (3, 9930)\t0.4630616107096361\n",
      "  (4, 431)\t0.2953073795144761\n",
      "  (4, 1669)\t0.37555085847053143\n",
      "  (4, 3917)\t0.27602557009293915\n",
      "  (4, 5114)\t0.1864097313364237\n",
      "  (4, 7912)\t0.3366639879070339\n",
      "  (4, 9526)\t0.3017828784900291\n",
      "  (4, 9543)\t0.27942729649560083\n",
      "  (4, 9600)\t0.6150782393275305\n",
      "  (5, 1030)\t0.7208531114618266\n",
      "  (5, 5408)\t0.6930878672259408\n",
      "  (6, 2940)\t1.0\n",
      "  (7, 2271)\t0.5927784732873538\n",
      "  (7, 5114)\t0.2679292098632014\n",
      "  :\t:\n",
      "  (124493, 5577)\t0.4300348744252286\n",
      "  (124493, 6222)\t0.24143918888678192\n",
      "  (124494, 1660)\t0.393336640101101\n",
      "  (124494, 2724)\t0.37240878016285267\n",
      "  (124494, 4190)\t0.45525383723220303\n",
      "  (124494, 5854)\t0.393336640101101\n",
      "  (124494, 6538)\t0.30182771561003297\n",
      "  (124494, 7003)\t0.3811175014322774\n",
      "  (124494, 9973)\t0.3290557695020282\n",
      "  (124495, 252)\t0.6428483898564891\n",
      "  (124495, 6940)\t0.5388988460526388\n",
      "  (124495, 7417)\t0.5443656688128428\n",
      "  (124496, 1890)\t0.4124299772792365\n",
      "  (124496, 2111)\t0.37450552494886663\n",
      "  (124496, 2921)\t0.29165980820546805\n",
      "  (124496, 6723)\t0.5054622893758767\n",
      "  (124496, 8797)\t0.2762605406742642\n",
      "  (124496, 9035)\t0.2930069647789499\n",
      "  (124496, 9373)\t0.43233851109756566\n",
      "  (124497, 3536)\t0.5170274710209506\n",
      "  (124497, 3806)\t0.28213410698330393\n",
      "  (124497, 6514)\t0.4628122135749097\n",
      "  (124497, 6846)\t0.33486857306843704\n",
      "  (124497, 8334)\t0.3553002978843605\n",
      "  (124497, 9419)\t0.4477862569873036\n"
     ]
    }
   ],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# fit the vectorizer\n",
    "print('Fitting the vectorizer...')\n",
    "vectorizer.fit(data['reduced_clean_text'])\n",
    "\n",
    "# transform the text\n",
    "print('Transforming the text...')\n",
    "X = vectorizer.transform(data['text'])\n",
    "\n",
    "# save the data\n",
    "scipy.sparse.save_npz('../dataset/process_data/tfidf_convert_sparse.npz', X)\n",
    "\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
