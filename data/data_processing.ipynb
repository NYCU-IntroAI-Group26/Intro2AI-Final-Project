{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data access and clean text and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "def read_txt(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data from tweetval: https://github.com/cardiffnlp/tweeteval/tree/main/datasets/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['../dataset/tweetval/test_text.txt', '../dataset/tweetval/test_labels.txt']\n",
    "train_files = ['../dataset/tweetval/train_text.txt', '../dataset/tweetval/train_labels.txt']\n",
    "val_files = ['../dataset/tweetval/val_text.txt', '../dataset/tweetval/val_labels.txt']\n",
    "\n",
    "test_text = read_txt(test_files[0])\n",
    "test_labels = read_txt(test_files[1])\n",
    "train_text = read_txt(train_files[0])\n",
    "train_labels = read_txt(train_files[1])\n",
    "val_text = read_txt(val_files[0])\n",
    "val_labels = read_txt(val_files[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "def create_df(text, labels):\n",
    "    df = pd.DataFrame({'text': text, 'sentiment': labels})\n",
    "    return df\n",
    "\n",
    "# cocatenate train, val and test data\n",
    "train_df = create_df(train_text, train_labels)\n",
    "val_df = create_df(val_text, val_labels)\n",
    "test_df = create_df(test_text, test_labels)\n",
    "\n",
    "new_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# save the dataframe as tsv file\n",
    "new_df.to_csv('../dataset/tweetval/tweetval_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove url\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)     # remove @\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)     # remove hashtag\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)   # remove \\n and \\t\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # remove extra whitespace\n",
    "    text = re.sub(r\"RT\", \"\", text)       # remove RT\n",
    "    text = re.sub(r\"pic.\\S+\", \"\", text)  # remove pic\n",
    "    text.strip()  # remove leading and trailing whitespace \n",
    "    text = text.lower()  # convert to lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "merged_raw_data  = pd.read_csv('../dataset/tweetval/tweetval_data.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  \"qt in the original draft of the 7th book, rem...          2\n",
      "1  \"ben smith / smith (concussion) remains out of...          1\n",
      "2  sorry bout the stream last night i crashed out...          1\n",
      "3  chase headley's rbi double in the 8th inning o...          1\n",
      "4   alciato: bee will invest 150 million in janua...          2\n"
     ]
    }
   ],
   "source": [
    "# remove the link or url in the text\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(clean_text)\n",
    "\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/tweetval/tweetval_data_cleaned.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  sentiment\n",
      "0  \"qt in the original draft of the 7th book, rem...          2\n",
      "1  \"ben smith / smith (concussion) remains out of...          1\n",
      "2  sorry bout the stream last night i crashed out...          1\n",
      "3  chase headley's rbi double in the 8th inning o...          1\n",
      "4   alciato: bee will invest 0 million in january...          2\n"
     ]
    }
   ],
   "source": [
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoji)\n",
    "\n",
    "# remove emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(r'[\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]√ß\\+œâ‚óã\\@¬°√©ƒ±„Éª‚Ä¶¬°\\`ÔºöÔºâ‚ô°”≥ÔºÅ‚Äú‚Äù√†‚âß‚àá‚â¶‚ôÇ≈ü‚âà¬¨‚äÑ‚îÄ‚úî‚Ä¢√ó√º‚Äì‚Çπ„ÄÇ√≥¬∞ ñ‚Äî¬∂ƒ∑√±‡∏øƒ∫‚àëÔºõ‚è∏][\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]√ß\\+œâ‚óã\\@¬°√©ƒ±„Éª‚Ä¶¬°\\`ÔºöÔºâ‚ô°”≥ÔºÅ‚Äú‚Äù√†‚âß‚àá‚â¶‚ôÇ≈ü‚âà¬¨‚äÑ‚îÄ‚úî‚Ä¢√ó√º‚Äì‚Çπ„ÄÇ√≥¬∞ ñ‚Äî¬∂ƒ∑√±‡∏øƒ∫‚àëÔºõ‚è∏]')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoticons)\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/tweetval/tweetval_data_cleaned_without_emoji_emoticons.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get another data from this dataset: https://github.com/BaleChen/emoji-setiment-analysis/tree/main/Data/emoji2vec_original_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetTrainingExample:\n",
    "    \"\"\"Structure holding a Tweet Training example\"\"\"\n",
    "\n",
    "    def __init__(self, id, text, label):\n",
    "        \"\"\"Create the training example\n",
    "\n",
    "        Args:\n",
    "            id: ID of the example\n",
    "            text: text of the example\n",
    "            label: example label\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str.format('{}, {}, {}\\n', self.id, self.label, self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"examples.p\"]\n",
    "data = defaultdict(list)\n",
    "\n",
    "for file in files:\n",
    "    with open(f'../dataset/example/{file}', 'rb') as f:\n",
    "        examples = pickle.load(f)\n",
    "        for example in examples.values():\n",
    "            data['label'].append(example.label)\n",
    "            data['text'].append(example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64599\n"
     ]
    }
   ],
   "source": [
    "df = df[df['text'].notnull()]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Neutral     29722\n",
      "Positive    18611\n",
      "Negative    16266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the number of negative, neutral and positive tweets\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "df['sentiment'] = df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          label                                               text  sentiment\n",
      "0      Negative              @alexsnowden_ what the hell snowballs          0\n",
      "1      Positive  RT @5SOS: Our bus is 100% rock n roll http://t...          2\n",
      "2      Positive  RT @InnocentLaTre: The game has changed üòÇüíÄ htt...          2\n",
      "3       Neutral  RT @TRILOGILINSKY: YO IS THIS A VINE HES SO FU...          1\n",
      "4      Negative  I hate wearing clothes like i just wanna walk ...          0\n",
      "...         ...                                                ...        ...\n",
      "64594  Positive  Mabaiiiit sobraaa. And then a sweet one! ‚ò∫Ô∏è‚ò∫Ô∏è ...          2\n",
      "64595   Neutral  The truth about education spending - The Progr...          1\n",
      "64596  Positive     @LaciePassmore lol Jeremy is bootylicious dude          2\n",
      "64597  Negative  @MannanJamil17 so am I hopefully you lot will ...          0\n",
      "64598  Positive  The whole time we're just laughing talking bou...          2\n",
      "\n",
      "[64599 rows x 3 columns]\n",
      "sentiment\n",
      "1    29722\n",
      "2    18611\n",
      "0    16266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "\n",
    "#  count the number of negative, neutral and positive tweets  \n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1    27479\n",
      "2    21043\n",
      "0    11377\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read the tsv file\n",
    "merged_raw_data  = pd.read_csv('../dataset/tweetval/tweetval_data.tsv', sep='\\t')\n",
    "\n",
    "#  count the number of negative, neutral and positive tweets\n",
    "print(merged_raw_data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          label                                               text  sentiment\n",
      "0      Negative              @alexsnowden_ what the hell snowballs          0\n",
      "1      Positive  RT @5SOS: Our bus is 100% rock n roll http://t...          2\n",
      "2      Positive  RT @InnocentLaTre: The game has changed üòÇüíÄ htt...          2\n",
      "3       Neutral  RT @TRILOGILINSKY: YO IS THIS A VINE HES SO FU...          1\n",
      "4      Negative  I hate wearing clothes like i just wanna walk ...          0\n",
      "...         ...                                                ...        ...\n",
      "59894       NaN  Sentinel Editorial: FBI‚Äôs Comey ‚Äòhad no one of...          1\n",
      "59895       NaN  perfect pussy clips #vanessa hudgens zac efron...          1\n",
      "59896       NaN  #latestnews 4 #newmexico #politics + #nativeam...          1\n",
      "59897       NaN  Trying to have a conversation with my dad abou...          0\n",
      "59898       NaN  @user You are a stand up guy and a Gentleman V...          2\n",
      "\n",
      "[124498 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# read the tsv file\n",
    "merged_raw_data  = pd.read_csv('../dataset/tweetval/tweetval_data.tsv', sep='\\t')\n",
    "\n",
    "# concat the two dataframes\n",
    "new_df = pd.concat([df, merged_raw_data], axis=0)\n",
    "\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "new_df['label'] = new_df['sentiment'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../dataset/process/tweets.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text  sentiment  \\\n",
      "0  Negative              @alexsnowden_ what the hell snowballs          0   \n",
      "1  Positive  RT @5SOS: Our bus is 100% rock n roll http://t...          2   \n",
      "2  Positive  RT @InnocentLaTre: The game has changed üòÇüíÄ htt...          2   \n",
      "3   Neutral  RT @TRILOGILINSKY: YO IS THIS A VINE HES SO FU...          1   \n",
      "4  Negative  I hate wearing clothes like i just wanna walk ...          0   \n",
      "\n",
      "   has_emoji  \n",
      "0      False  \n",
      "1      False  \n",
      "2       True  \n",
      "3      False  \n",
      "4      False  \n",
      "has_emoji\n",
      "False    113065\n",
      "True      11433\n",
      "Name: count, dtype: int64\n",
      "['RT @InnocentLaTre: The game has changed üòÇüíÄ http://t.co/4TqPV5eqCY'\n",
      " '‚Äú@Tvga_: @theninimarley bootayyyyyy !!!‚Äùmassive booty üòç'\n",
      " 'Like can I just ignore you üòÇüòÇüòÇüòÇ'\n",
      " '@Bon_Qui_Quii lmaoo akuaaaaa üò≠üò≠ we both are going to mss'\n",
      " 'RT @wOwBoice: üòçüòçüòçüòçüòç\"@boice106: #CSPH  Greedy man\\nCr.ahebrewprincess\\nhttp://t.co/MdeBsAycKF\"'\n",
      " 'Chelsea just fell off her chair üòÇüòÇüëèüëè'\n",
      " '@Lisa_higgins0 that was supposed to be to u but Idek wtfüòÇüòÇ'\n",
      " 'Can somebody talk to me üåö' '@37CraicsOfTheo AWWW ‚ù§‚ù§‚ù§ THANKS BBY üíï'\n",
      " \"@NiallOfficial Hi sunshine! You're my everything.Thank you for you changed my life.Can you please follow me?It's my biggest dream.üíó x7,897\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def count_emoji(text):\n",
    "    # Use regex to find all Unicode emoji characters\n",
    "    emoji_pattern = re.compile(r'([\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF])')\n",
    "    emojis = emoji_pattern.findall(text)\n",
    "    \n",
    "    # If the list of emojis is not empty, the text contains an emoji\n",
    "    return bool(emojis)\n",
    "\n",
    "new_df[\"has_emoji\"] = new_df[\"text\"].apply(count_emoji)\n",
    "print(new_df.head())\n",
    "\n",
    "print(new_df[\"has_emoji\"].value_counts())\n",
    "\n",
    "# print text with emoji\n",
    "print(new_df[new_df[\"has_emoji\"] == True][\"text\"].values[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove url\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)     # remove @\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)     # remove hashtag\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)   # remove \\n and \\t\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # remove extra whitespace\n",
    "    text = re.sub(r\"RT\", \"\", text)       # remove RT\n",
    "    text = re.sub(r\"pic.\\S+\", \"\", text)  # remove pic\n",
    "    text.strip()  # remove leading and trailing whitespace \n",
    "    text = text.lower()  # convert to lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "merged_raw_data = pd.read_csv('../dataset/process/tweets_convert.tsv', sep='\\t')\n",
    "\n",
    "merged_raw_data ['text'] = merged_raw_data ['text'].astype(str)\n",
    "\n",
    "# remove the link or url in the text\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(clean_text)\n",
    "\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/tweets_convert_cleaned.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoji)\n",
    "\n",
    "# remove emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(r'[\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]√ß\\+œâ‚óã\\@¬°√©ƒ±„Éª‚Ä¶¬°\\`ÔºöÔºâ‚ô°”≥ÔºÅ‚Äú‚Äù√†‚âß‚àá‚â¶‚ôÇ≈ü‚âà¬¨‚äÑ‚îÄ‚úî‚Ä¢√ó√º‚Äì‚Çπ„ÄÇ√≥¬∞ ñ‚Äî¬∂ƒ∑√±‡∏øƒ∫‚àëÔºõ‚è∏][\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]√ß\\+œâ‚óã\\@¬°√©ƒ±„Éª‚Ä¶¬°\\`ÔºöÔºâ‚ô°”≥ÔºÅ‚Äú‚Äù√†‚âß‚àá‚â¶‚ôÇ≈ü‚âà¬¨‚äÑ‚îÄ‚úî‚Ä¢√ó√º‚Äì‚Çπ„ÄÇ√≥¬∞ ñ‚Äî¬∂ƒ∑√±‡∏øƒ∫‚àëÔºõ‚è∏]')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoticons)\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/tweets_convert_without_emoji_emoticons.tsv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
