{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_bz2(file_path):\n",
    "    with bz2.open(file_path, \"rt\") as file:\n",
    "        df = pd.read_csv(file, delimiter=\"\\t\", header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset source\n",
    "Yin, Wenjie; Alkhalifa, Rabab; Zubiaga, Arkaitz (2021). TM-Senti. figshare. Dataset. https://doi.org/10.6084/m9.figshare.16438281.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ./dataset/TM-Senti/en-ids.tsv.bz2:\n",
      "         date             tweetid sentiment evidence\n",
      "0  2013-01-01  286003826803240960       pos        ✌\n",
      "1  2013-01-01  286003826811625472       neg        😒\n",
      "2  2013-01-01  286003826815807489       pos        😊\n",
      "3  2013-01-01  286003830993326080       pos        😘\n",
      "4  2013-01-01  286003831001710592       pos       :)\n",
      "117411852\n",
      "\n",
      "\n",
      "                 date              tweetid sentiment evidence\n",
      "111100330  2020-01-01  1212267211261267969       pos        😇\n",
      "111100331  2020-01-01  1212267211282436097       pos        🥳\n",
      "111100332  2020-01-01  1212267211299151874       pos        🥳\n",
      "111100333  2020-01-01  1212267232245551104       neg      😭,😭\n",
      "111100334  2020-01-01  1212267244828418050       neg      😭,😭\n",
      "...               ...                  ...       ...      ...\n",
      "111132434  2020-01-01  1212629544617828352       pos        😘\n",
      "111132435  2020-01-01  1212629557204922368       neg      😭,😭\n",
      "111132436  2020-01-01  1212629565576601600       pos        😍\n",
      "111132437  2020-01-01  1212629565593604096       neg      😭,😭\n",
      "111132438  2020-01-01  1212629578163769344       pos        😊\n",
      "\n",
      "[32109 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    # \"../dataset/TM-Senti/ar-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/de-ids.tsv.bz2\",\n",
    "    \"../dataset/TM-Senti/en-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/es-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/fr-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/it-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/zh-ids.tsv.bz2\"\n",
    "]\n",
    "\n",
    "# read all datasets\n",
    "dfs = [read_tsv_bz2(file_path) for file_path in file_paths]\n",
    "\n",
    "# print the first 5 rows of each dataset\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Data from {file_paths[i]}:\")\n",
    "    # date \\t tweetid \\t sentiment \\t evidence\n",
    "    df.columns = [\"date\", \"tweetid\", \"sentiment\", \"evidence\"]\n",
    "    print(df.head())\n",
    "    print(len(df))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# print the df with the date \"2020-01-01\"\n",
    "print(df[df[\"date\"] == \"2020-01-01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date              tweetid sentiment evidence\n",
      "111100330  2020-01-01  1212267211261267969       pos        😇\n",
      "111100331  2020-01-01  1212267211282436097       pos        🥳\n",
      "111100332  2020-01-01  1212267211299151874       pos        🥳\n",
      "111100333  2020-01-01  1212267232245551104       neg      😭,😭\n",
      "111100334  2020-01-01  1212267244828418050       neg      😭,😭\n",
      "942466\n"
     ]
    }
   ],
   "source": [
    "# save the 2020-01 data to a csv file\n",
    "en_2020_tweet_data = df[df[\"date\"].str.contains(\"2020-01\")].copy()\n",
    "\n",
    "print(en_2020_tweet_data.head())\n",
    "print(len(en_2020_tweet_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date              tweetid  sentiment evidence\n",
      "0  2020-01-01  1212267211261267969          1        😇\n",
      "1  2020-01-01  1212267211282436097          1        🥳\n",
      "2  2020-01-01  1212267211299151874          1        🥳\n",
      "3  2020-01-01  1212267232245551104          0      😭,😭\n",
      "4  2020-01-01  1212267244828418050          0      😭,😭\n"
     ]
    }
   ],
   "source": [
    "# pos -> 1, neg -> 0\n",
    "en_2020_tweet_data[\"sentiment\"] = en_2020_tweet_data[\"sentiment\"].apply(lambda x: 1 if x == \"pos\" else 0)\n",
    "\n",
    "# rearrange index\n",
    "en_2020_tweet_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(en_2020_tweet_data.head())\n",
    "# save to tsv\n",
    "en_2020_tweet_data.to_csv(\"../dataset/TM-Senti/en-2020-01.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add orign tweet data\n",
    "\n",
    "Concat parts of TM-Senti data with its origin tweet(2020-01-01~2020-01-05 with 2020-01-03 excluded due to missing soure tweets data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942466\n"
     ]
    }
   ],
   "source": [
    "en_2020_01_TM_Senti = pd.read_csv(\"../dataset/TM-Senti/en-2020-01.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(len(en_2020_01_TM_Senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_data_2020_01_01.csv\n",
      "tweets_data_2020_01_02.csv\n",
      "tweets_data_2020_01_04.csv\n",
      "tweets_data_2020_01_05.csv\n"
     ]
    }
   ],
   "source": [
    "# get all the tweet csv file in folder /dataset/tweets_data\n",
    "tweet_files = os.listdir(\"../dataset/tweets_data\")\n",
    "tweet_files = [file for file in tweet_files if file.endswith(\".csv\")]\n",
    "en_2020_0101_tweet_raw_data = pd.DataFrame()\n",
    "for file in tweet_files:\n",
    "    print(file)\n",
    "    df = pd.read_csv(f\"../dataset/tweets_data/{file}\")\n",
    "    en_2020_0101_tweet_raw_data = pd.concat([en_2020_0101_tweet_raw_data, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text\n",
      "0  1212267211265458176                    ガチャ結果によってはサプでタミン取ろうかと思ったけどダマがない\n",
      "1  1212267211261280256         Kind Lady(interlude) / OKUYATOS/ CSDDRMAX2\n",
      "2  1212267211290624001                                          ワイ、三国11連敗\n",
      "3  1212267211261267969  Look y’all a bitch going to sleep. Happy New Y...\n",
      "4  1212267211298983939  RT @icedflatwhite: ถ้าปีหน้าเผลอไปด่าใครก็นั่น...\n",
      "         date              tweetid  sentiment evidence\n",
      "0  2020-01-01  1212267211261267969          1        😇\n",
      "1  2020-01-01  1212267211282436097          1        🥳\n",
      "2  2020-01-01  1212267211299151874          1        🥳\n",
      "3  2020-01-01  1212267232245551104          0      😭,😭\n",
      "4  2020-01-01  1212267244828418050          0      😭,😭\n"
     ]
    }
   ],
   "source": [
    "print(en_2020_0101_tweet_raw_data.head())\n",
    "print(en_2020_01_TM_Senti.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date              tweetid  sentiment evidence  \\\n",
      "0  2020-01-01  1212267211261267969          1        😇   \n",
      "1  2020-01-01  1212267211282436097          1        🥳   \n",
      "2  2020-01-01  1212267211299151874          1        🥳   \n",
      "3  2020-01-01  1212267232245551104          0      😭,😭   \n",
      "4  2020-01-01  1212267244828418050          0      😭,😭   \n",
      "\n",
      "                                                text  \n",
      "0  Look y’all a bitch going to sleep. Happy New Y...  \n",
      "1  To our Mountain time Moose Chucklers:\\n\\n🏔️🥳HA...  \n",
      "2  Happy New Year, Denver! 🥳🥂 https://t.co/xo9qCT...  \n",
      "3  @BTS_twt yoongi i love you so so so so so so s...  \n",
      "4                           What an amazing night! 😭  \n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(en_2020_01_TM_Senti, en_2020_0101_tweet_raw_data, left_on=\"tweetid\", right_on=\"id\")\n",
    "merged_data.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=[\"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate null or duplicate values\n",
    "merged_data.dropna(inplace=True)\n",
    "merged_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_data))\n",
    "\n",
    "merged_data.to_csv(\"../dataset/process/en-2020-01-merged.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove url\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)     # remove @\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)     # remove hashtag\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)   # remove \\n and \\t\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # remove extra whitespace\n",
    "    text = re.sub(r\"RT\", \"\", text)       # remove RT\n",
    "    text = re.sub(r\"pic.\\S+\", \"\", text)  # remove pic\n",
    "    text.strip()  # remove leading and trailing whitespace \n",
    "    text = text.lower()  # convert to lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_raw_data = pd.read_csv(\"../dataset/process/en-2020-01-merged.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweetid  sentiment evidence  \\\n",
      "0  1212267211261267969          1        😇   \n",
      "1  1212267211282436097          1        🥳   \n",
      "2  1212267211299151874          1        🥳   \n",
      "3  1212267232245551104          0      😭,😭   \n",
      "4  1212267244828418050          0      😭,😭   \n",
      "\n",
      "                                                text  \n",
      "0  look y’all a bitch going to sleep. happy new y...  \n",
      "1  to our mountain time moose chucklers: 🏔️🥳happy...  \n",
      "2                        happy new year, denver! 🥳🥂   \n",
      "3   yoongi i love you so so so so so so so so so ...  \n",
      "4                           what an amazing night! 😭  \n"
     ]
    }
   ],
   "source": [
    "# remove the link or url in the text\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(clean_text)\n",
    "\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/en-2020-01-merged-cleaned.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129670\n",
      "0\n",
      "1743\n",
      "['look y’all a bitch going to sleep. happy new years tho 😇'\n",
      " 'to our mountain time moose chucklers: 🏔️🥳happy new year! 🥳🏔️ from all of us at ! '\n",
      " 'happy new year, denver! 🥳🥂 '\n",
      " ' yoongi i love you so so so so so so so so so much 🥺😭💗💓💞💝💘💕💖💝💞💓💗💘💕💖 '\n",
      " 'what an amazing night! 😭'\n",
      " ' happy new year!! 🎆🎆🎉🎉 thank you for sharing the beautiful art!🥰🥰'\n",
      " ' happy birthday b !! i hope you had a good new year and i hope your birthday goes well! 🥳🥳🎊'\n",
      " 'happy new year, everyone!!! 🥳 - 2 piggys 🤪 ' 'it’s new years🥳'\n",
      " 'happy new year 🥳 ']\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_raw_data))\n",
    "print(merged_raw_data[\"text\"].isnull().sum())\n",
    "print(merged_raw_data[\"text\"].duplicated().sum())\n",
    "print(merged_raw_data[\"text\"].values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweetid  sentiment evidence  \\\n",
      "0  1212267211261267969          1        😇   \n",
      "1  1212267211282436097          1        🥳   \n",
      "2  1212267211299151874          1        🥳   \n",
      "3  1212267232245551104          0      😭,😭   \n",
      "4  1212267244828418050          0      😭,😭   \n",
      "\n",
      "                                                text  \n",
      "0  look y’all a bitch going to sleep. happy new y...  \n",
      "1  to our mountain time moose chucklers: happy ne...  \n",
      "2                          happy new year, denver!    \n",
      "3   yoongi i love you so so so so so so so so so ...  \n",
      "4                            what an amazing night!   \n"
     ]
    }
   ],
   "source": [
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoji)\n",
    "\n",
    "# remove emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(r'[\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]ç\\+ω○\\@¡éı・…¡\\`：）♡ӳ！“”à≧∇≦♂ş≈¬⊄─✔•×ü–₹。ó°ʖ—¶ķñ฿ĺ∑；⏸][\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]ç\\+ω○\\@¡éı・…¡\\`：）♡ӳ！“”à≧∇≦♂ş≈¬⊄─✔•×ü–₹。ó°ʖ—¶ķñ฿ĺ∑；⏸]')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoticons)\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/en-2020-01-merged-cleaned-without-emoji.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 '😇' 'look y’all a bitch going to sleep. happy new years tho 😇']\n",
      " [1 '🥳'\n",
      "  'to our mountain time moose chucklers: 🏔️🥳happy new year! 🥳🏔️ from all of us at ! ']\n",
      " [1 '🥳' 'happy new year, denver! 🥳🥂 ']\n",
      " [0 '😭,😭'\n",
      "  ' yoongi i love you so so so so so so so so so much 🥺😭💗💓💞💝💘💕💖💝💞💓💗💘💕💖 ']\n",
      " [0 '😭,😭' 'what an amazing night! 😭']\n",
      " [1 '🥰'\n",
      "  ' happy new year!! 🎆🎆🎉🎉 thank you for sharing the beautiful art!🥰🥰']\n",
      " [1 '🥳'\n",
      "  ' happy birthday b !! i hope you had a good new year and i hope your birthday goes well! 🥳🥳🎊']\n",
      " [1 '🥳' 'happy new year, everyone!!! 🥳 - 2 piggys 🤪 ']\n",
      " [1 '🥳' 'it’s new years🥳']\n",
      " [1 '🥳' 'happy new year 🥳 ']\n",
      " [1 '😊,😍'\n",
      "  '😊 happy new year for real. the time is here. thought i’d be sleeping by now.🎉🎊😍🎈⭐️🕸🐾❄️💫']\n",
      " [0 '😭,😭'\n",
      "  'i’m so done up i keep opening instagram thinking it’s twitter 😭😭']\n",
      " [1 '👍' 'we support you 👍🏽']\n",
      " [1 '🥳' 'hello 2020. 🥳']\n",
      " [1 '😌,😌' 'im going thru it 😌😌']]\n"
     ]
    }
   ],
   "source": [
    "data_clean = pd.read_csv(\"../dataset/process/en-2020-01-merged-cleaned.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(data_clean[['sentiment', 'evidence', 'text']][0:15].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1    66006\n",
      "0    63664\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "1    0.509031\n",
      "0    0.490969\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count the number of positive and negative sentiment\n",
    "print(data_clean[\"sentiment\"].value_counts())\n",
    "\n",
    "# count the number of positive and negative sentiment\n",
    "print(data_clean[\"sentiment\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 124709\n",
      "Number of unique emoji: 1015\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique words\n",
    "\n",
    "unique_words = set()\n",
    "data_clean['text'].astype(str).apply(lambda x: unique_words.update(x.split()))\n",
    "print(f\"Number of unique words: {len(unique_words)}\")\n",
    "\n",
    "# count the number of unique emoji\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "unique_emoji = set()\n",
    "data_clean['text'].astype(str).apply(lambda x: unique_emoji.update(extract_emojis(x)))\n",
    "print(f\"Number of unique emoji: {len(unique_emoji)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
