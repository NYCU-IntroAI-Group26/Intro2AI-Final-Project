{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_bz2(file_path):\n",
    "    with bz2.open(file_path, \"rt\") as file:\n",
    "        df = pd.read_csv(file, delimiter=\"\\t\", header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset source\n",
    "Yin, Wenjie; Alkhalifa, Rabab; Zubiaga, Arkaitz (2021). TM-Senti. figshare. Dataset. https://doi.org/10.6084/m9.figshare.16438281.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ./dataset/TM-Senti/en-ids.tsv.bz2:\n",
      "         date             tweetid sentiment evidence\n",
      "0  2013-01-01  286003826803240960       pos        âœŒ\n",
      "1  2013-01-01  286003826811625472       neg        ğŸ˜’\n",
      "2  2013-01-01  286003826815807489       pos        ğŸ˜Š\n",
      "3  2013-01-01  286003830993326080       pos        ğŸ˜˜\n",
      "4  2013-01-01  286003831001710592       pos       :)\n",
      "117411852\n",
      "\n",
      "\n",
      "                 date              tweetid sentiment evidence\n",
      "111100330  2020-01-01  1212267211261267969       pos        ğŸ˜‡\n",
      "111100331  2020-01-01  1212267211282436097       pos        ğŸ¥³\n",
      "111100332  2020-01-01  1212267211299151874       pos        ğŸ¥³\n",
      "111100333  2020-01-01  1212267232245551104       neg      ğŸ˜­,ğŸ˜­\n",
      "111100334  2020-01-01  1212267244828418050       neg      ğŸ˜­,ğŸ˜­\n",
      "...               ...                  ...       ...      ...\n",
      "111132434  2020-01-01  1212629544617828352       pos        ğŸ˜˜\n",
      "111132435  2020-01-01  1212629557204922368       neg      ğŸ˜­,ğŸ˜­\n",
      "111132436  2020-01-01  1212629565576601600       pos        ğŸ˜\n",
      "111132437  2020-01-01  1212629565593604096       neg      ğŸ˜­,ğŸ˜­\n",
      "111132438  2020-01-01  1212629578163769344       pos        ğŸ˜Š\n",
      "\n",
      "[32109 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    # \"../dataset/TM-Senti/ar-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/de-ids.tsv.bz2\",\n",
    "    \"../dataset/TM-Senti/en-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/es-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/fr-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/it-ids.tsv.bz2\",\n",
    "    # \"../dataset/TM-Senti/zh-ids.tsv.bz2\"\n",
    "]\n",
    "\n",
    "# read all datasets\n",
    "dfs = [read_tsv_bz2(file_path) for file_path in file_paths]\n",
    "\n",
    "# print the first 5 rows of each dataset\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Data from {file_paths[i]}:\")\n",
    "    # date \\t tweetid \\t sentiment \\t evidence\n",
    "    df.columns = [\"date\", \"tweetid\", \"sentiment\", \"evidence\"]\n",
    "    print(df.head())\n",
    "    print(len(df))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# print the df with the date \"2020-01-01\"\n",
    "print(df[df[\"date\"] == \"2020-01-01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date              tweetid sentiment evidence\n",
      "111100330  2020-01-01  1212267211261267969       pos        ğŸ˜‡\n",
      "111100331  2020-01-01  1212267211282436097       pos        ğŸ¥³\n",
      "111100332  2020-01-01  1212267211299151874       pos        ğŸ¥³\n",
      "111100333  2020-01-01  1212267232245551104       neg      ğŸ˜­,ğŸ˜­\n",
      "111100334  2020-01-01  1212267244828418050       neg      ğŸ˜­,ğŸ˜­\n",
      "942466\n"
     ]
    }
   ],
   "source": [
    "# save the 2020-01 data to a csv file\n",
    "en_2020_tweet_data = df[df[\"date\"].str.contains(\"2020-01\")].copy()\n",
    "\n",
    "print(en_2020_tweet_data.head())\n",
    "print(len(en_2020_tweet_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date              tweetid  sentiment evidence\n",
      "0  2020-01-01  1212267211261267969          1        ğŸ˜‡\n",
      "1  2020-01-01  1212267211282436097          1        ğŸ¥³\n",
      "2  2020-01-01  1212267211299151874          1        ğŸ¥³\n",
      "3  2020-01-01  1212267232245551104          0      ğŸ˜­,ğŸ˜­\n",
      "4  2020-01-01  1212267244828418050          0      ğŸ˜­,ğŸ˜­\n"
     ]
    }
   ],
   "source": [
    "# pos -> 1, neg -> 0\n",
    "en_2020_tweet_data[\"sentiment\"] = en_2020_tweet_data[\"sentiment\"].apply(lambda x: 1 if x == \"pos\" else 0)\n",
    "\n",
    "# rearrange index\n",
    "en_2020_tweet_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(en_2020_tweet_data.head())\n",
    "# save to tsv\n",
    "en_2020_tweet_data.to_csv(\"../dataset/TM-Senti/en-2020-01.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add orign tweet data\n",
    "\n",
    "Concat parts of TM-Senti data with its origin tweet(2020-01-01~2020-01-05 with 2020-01-03 excluded due to missing soure tweets data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942466\n"
     ]
    }
   ],
   "source": [
    "en_2020_01_TM_Senti = pd.read_csv(\"../dataset/TM-Senti/en-2020-01.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(len(en_2020_01_TM_Senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_data_2020_01_01.csv\n",
      "tweets_data_2020_01_02.csv\n",
      "tweets_data_2020_01_04.csv\n",
      "tweets_data_2020_01_05.csv\n"
     ]
    }
   ],
   "source": [
    "# get all the tweet csv file in folder /dataset/tweets_data\n",
    "tweet_files = os.listdir(\"../dataset/tweets_data\")\n",
    "tweet_files = [file for file in tweet_files if file.endswith(\".csv\")]\n",
    "en_2020_0101_tweet_raw_data = pd.DataFrame()\n",
    "for file in tweet_files:\n",
    "    print(file)\n",
    "    df = pd.read_csv(f\"../dataset/tweets_data/{file}\")\n",
    "    en_2020_0101_tweet_raw_data = pd.concat([en_2020_0101_tweet_raw_data, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text\n",
      "0  1212267211265458176                    ã‚¬ãƒãƒ£çµæœã«ã‚ˆã£ã¦ã¯ã‚µãƒ—ã§ã‚¿ãƒŸãƒ³å–ã‚ã†ã‹ã¨æ€ã£ãŸã‘ã©ãƒ€ãƒãŒãªã„\n",
      "1  1212267211261280256         Kind Lady(interlude) / OKUYATOS/ CSDDRMAX2\n",
      "2  1212267211290624001                                          ãƒ¯ã‚¤ã€ä¸‰å›½11é€£æ•—\n",
      "3  1212267211261267969  Look yâ€™all a bitch going to sleep. Happy New Y...\n",
      "4  1212267211298983939  RT @icedflatwhite: à¸–à¹‰à¸²à¸›à¸µà¸«à¸™à¹‰à¸²à¹€à¸œà¸¥à¸­à¹„à¸›à¸”à¹ˆà¸²à¹ƒà¸„à¸£à¸à¹‡à¸™à¸±à¹ˆà¸™...\n",
      "         date              tweetid  sentiment evidence\n",
      "0  2020-01-01  1212267211261267969          1        ğŸ˜‡\n",
      "1  2020-01-01  1212267211282436097          1        ğŸ¥³\n",
      "2  2020-01-01  1212267211299151874          1        ğŸ¥³\n",
      "3  2020-01-01  1212267232245551104          0      ğŸ˜­,ğŸ˜­\n",
      "4  2020-01-01  1212267244828418050          0      ğŸ˜­,ğŸ˜­\n"
     ]
    }
   ],
   "source": [
    "print(en_2020_0101_tweet_raw_data.head())\n",
    "print(en_2020_01_TM_Senti.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date              tweetid  sentiment evidence  \\\n",
      "0  2020-01-01  1212267211261267969          1        ğŸ˜‡   \n",
      "1  2020-01-01  1212267211282436097          1        ğŸ¥³   \n",
      "2  2020-01-01  1212267211299151874          1        ğŸ¥³   \n",
      "3  2020-01-01  1212267232245551104          0      ğŸ˜­,ğŸ˜­   \n",
      "4  2020-01-01  1212267244828418050          0      ğŸ˜­,ğŸ˜­   \n",
      "\n",
      "                                                text  \n",
      "0  Look yâ€™all a bitch going to sleep. Happy New Y...  \n",
      "1  To our Mountain time Moose Chucklers:\\n\\nğŸ”ï¸ğŸ¥³HA...  \n",
      "2  Happy New Year, Denver! ğŸ¥³ğŸ¥‚ https://t.co/xo9qCT...  \n",
      "3  @BTS_twt yoongi i love you so so so so so so s...  \n",
      "4                           What an amazing night! ğŸ˜­  \n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(en_2020_01_TM_Senti, en_2020_0101_tweet_raw_data, left_on=\"tweetid\", right_on=\"id\")\n",
    "merged_data.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=[\"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate null or duplicate values\n",
    "merged_data.dropna(inplace=True)\n",
    "merged_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_data))\n",
    "\n",
    "merged_data.to_csv(\"../dataset/process/en-2020-01-merged.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove url\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)     # remove @\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)     # remove hashtag\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)   # remove \\n and \\t\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # remove extra whitespace\n",
    "    text = re.sub(r\"RT\", \"\", text)       # remove RT\n",
    "    text = re.sub(r\"pic.\\S+\", \"\", text)  # remove pic\n",
    "    text.strip()  # remove leading and trailing whitespace \n",
    "    text = text.lower()  # convert to lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_raw_data = pd.read_csv(\"../dataset/process/en-2020-01-merged.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweetid  sentiment evidence  \\\n",
      "0  1212267211261267969          1        ğŸ˜‡   \n",
      "1  1212267211282436097          1        ğŸ¥³   \n",
      "2  1212267211299151874          1        ğŸ¥³   \n",
      "3  1212267232245551104          0      ğŸ˜­,ğŸ˜­   \n",
      "4  1212267244828418050          0      ğŸ˜­,ğŸ˜­   \n",
      "\n",
      "                                                text  \n",
      "0  look yâ€™all a bitch going to sleep. happy new y...  \n",
      "1  to our mountain time moose chucklers: ğŸ”ï¸ğŸ¥³happy...  \n",
      "2                        happy new year, denver! ğŸ¥³ğŸ¥‚   \n",
      "3   yoongi i love you so so so so so so so so so ...  \n",
      "4                           what an amazing night! ğŸ˜­  \n"
     ]
    }
   ],
   "source": [
    "# remove the link or url in the text\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(clean_text)\n",
    "\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/en-2020-01-merged-cleaned.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129670\n",
      "0\n",
      "1743\n",
      "['look yâ€™all a bitch going to sleep. happy new years tho ğŸ˜‡'\n",
      " 'to our mountain time moose chucklers: ğŸ”ï¸ğŸ¥³happy new year! ğŸ¥³ğŸ”ï¸ from all of us at ! '\n",
      " 'happy new year, denver! ğŸ¥³ğŸ¥‚ '\n",
      " ' yoongi i love you so so so so so so so so so much ğŸ¥ºğŸ˜­ğŸ’—ğŸ’“ğŸ’ğŸ’ğŸ’˜ğŸ’•ğŸ’–ğŸ’ğŸ’ğŸ’“ğŸ’—ğŸ’˜ğŸ’•ğŸ’– '\n",
      " 'what an amazing night! ğŸ˜­'\n",
      " ' happy new year!! ğŸ†ğŸ†ğŸ‰ğŸ‰ thank you for sharing the beautiful art!ğŸ¥°ğŸ¥°'\n",
      " ' happy birthday b !! i hope you had a good new year and i hope your birthday goes well! ğŸ¥³ğŸ¥³ğŸŠ'\n",
      " 'happy new year, everyone!!! ğŸ¥³ - 2 piggys ğŸ¤ª ' 'itâ€™s new yearsğŸ¥³'\n",
      " 'happy new year ğŸ¥³ ']\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_raw_data))\n",
    "print(merged_raw_data[\"text\"].isnull().sum())\n",
    "print(merged_raw_data[\"text\"].duplicated().sum())\n",
    "print(merged_raw_data[\"text\"].values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweetid  sentiment evidence  \\\n",
      "0  1212267211261267969          1        ğŸ˜‡   \n",
      "1  1212267211282436097          1        ğŸ¥³   \n",
      "2  1212267211299151874          1        ğŸ¥³   \n",
      "3  1212267232245551104          0      ğŸ˜­,ğŸ˜­   \n",
      "4  1212267244828418050          0      ğŸ˜­,ğŸ˜­   \n",
      "\n",
      "                                                text  \n",
      "0  look yâ€™all a bitch going to sleep. happy new y...  \n",
      "1  to our mountain time moose chucklers: happy ne...  \n",
      "2                          happy new year, denver!    \n",
      "3   yoongi i love you so so so so so so so so so ...  \n",
      "4                            what an amazing night!   \n"
     ]
    }
   ],
   "source": [
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoji)\n",
    "\n",
    "# remove emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(r'[\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]Ã§\\+Ï‰â—‹\\@Â¡Ã©Ä±ãƒ»â€¦Â¡\\`ï¼šï¼‰â™¡Ó³ï¼â€œâ€Ã â‰§âˆ‡â‰¦â™‚ÅŸâ‰ˆÂ¬âŠ„â”€âœ”â€¢Ã—Ã¼â€“â‚¹ã€‚Ã³Â°Ê–â€”Â¶Ä·Ã±à¸¿Äºâˆ‘ï¼›â¸][\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]Ã§\\+Ï‰â—‹\\@Â¡Ã©Ä±ãƒ»â€¦Â¡\\`ï¼šï¼‰â™¡Ó³ï¼â€œâ€Ã â‰§âˆ‡â‰¦â™‚ÅŸâ‰ˆÂ¬âŠ„â”€âœ”â€¢Ã—Ã¼â€“â‚¹ã€‚Ã³Â°Ê–â€”Â¶Ä·Ã±à¸¿Äºâˆ‘ï¼›â¸]')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "merged_raw_data[\"text\"] = merged_raw_data[\"text\"].apply(remove_emoticons)\n",
    "print(merged_raw_data.head())\n",
    "\n",
    "merged_raw_data.to_csv(\"../dataset/process/en-2020-01-merged-cleaned-without-emoji.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 'ğŸ˜‡' 'look yâ€™all a bitch going to sleep. happy new years tho ğŸ˜‡']\n",
      " [1 'ğŸ¥³'\n",
      "  'to our mountain time moose chucklers: ğŸ”ï¸ğŸ¥³happy new year! ğŸ¥³ğŸ”ï¸ from all of us at ! ']\n",
      " [1 'ğŸ¥³' 'happy new year, denver! ğŸ¥³ğŸ¥‚ ']\n",
      " [0 'ğŸ˜­,ğŸ˜­'\n",
      "  ' yoongi i love you so so so so so so so so so much ğŸ¥ºğŸ˜­ğŸ’—ğŸ’“ğŸ’ğŸ’ğŸ’˜ğŸ’•ğŸ’–ğŸ’ğŸ’ğŸ’“ğŸ’—ğŸ’˜ğŸ’•ğŸ’– ']\n",
      " [0 'ğŸ˜­,ğŸ˜­' 'what an amazing night! ğŸ˜­']\n",
      " [1 'ğŸ¥°'\n",
      "  ' happy new year!! ğŸ†ğŸ†ğŸ‰ğŸ‰ thank you for sharing the beautiful art!ğŸ¥°ğŸ¥°']\n",
      " [1 'ğŸ¥³'\n",
      "  ' happy birthday b !! i hope you had a good new year and i hope your birthday goes well! ğŸ¥³ğŸ¥³ğŸŠ']\n",
      " [1 'ğŸ¥³' 'happy new year, everyone!!! ğŸ¥³ - 2 piggys ğŸ¤ª ']\n",
      " [1 'ğŸ¥³' 'itâ€™s new yearsğŸ¥³']\n",
      " [1 'ğŸ¥³' 'happy new year ğŸ¥³ ']\n",
      " [1 'ğŸ˜Š,ğŸ˜'\n",
      "  'ğŸ˜Š happy new year for real. the time is here. thought iâ€™d be sleeping by now.ğŸ‰ğŸŠğŸ˜ğŸˆâ­ï¸ğŸ•¸ğŸ¾â„ï¸ğŸ’«']\n",
      " [0 'ğŸ˜­,ğŸ˜­'\n",
      "  'iâ€™m so done up i keep opening instagram thinking itâ€™s twitter ğŸ˜­ğŸ˜­']\n",
      " [1 'ğŸ‘' 'we support you ğŸ‘ğŸ½']\n",
      " [1 'ğŸ¥³' 'hello 2020. ğŸ¥³']\n",
      " [1 'ğŸ˜Œ,ğŸ˜Œ' 'im going thru it ğŸ˜ŒğŸ˜Œ']]\n"
     ]
    }
   ],
   "source": [
    "data_clean = pd.read_csv(\"../dataset/process/en-2020-01-merged-cleaned.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print(data_clean[['sentiment', 'evidence', 'text']][0:15].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1    66006\n",
      "0    63664\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "1    0.509031\n",
      "0    0.490969\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count the number of positive and negative sentiment\n",
    "print(data_clean[\"sentiment\"].value_counts())\n",
    "\n",
    "# count the number of positive and negative sentiment\n",
    "print(data_clean[\"sentiment\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 124709\n",
      "Number of unique emoji: 1015\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique words\n",
    "\n",
    "unique_words = set()\n",
    "data_clean['text'].astype(str).apply(lambda x: unique_words.update(x.split()))\n",
    "print(f\"Number of unique words: {len(unique_words)}\")\n",
    "\n",
    "# count the number of unique emoji\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "unique_emoji = set()\n",
    "data_clean['text'].astype(str).apply(lambda x: unique_emoji.update(extract_emojis(x)))\n",
    "print(f\"Number of unique emoji: {len(unique_emoji)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
