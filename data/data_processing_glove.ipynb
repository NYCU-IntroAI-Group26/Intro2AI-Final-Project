{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the necessary resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# set up the necessary resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def clean_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the spelling\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# remove the stop words and stem the words\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning the text: 100%|██████████| 129670/129670 [00:00<00:00, 180222.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing the text: 100%|██████████| 129670/129670 [00:09<00:00, 13370.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('../dataset/process/en-2020-01-merged-cleaned-without-emoji.tsv', sep='\\t')\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "print('Cleaning the text...')\n",
    "tqdm.pandas(desc=\"Cleaning the text\")\n",
    "data['text'] = data['text'].progress_apply(clean_punctuation)\n",
    "\n",
    "# print('Correcting the spelling...')\n",
    "# tqdm.pandas(desc=\"Correcting the spelling\")\n",
    "# data['text'] = data['text'].progress_apply(correct_spelling)\n",
    "\n",
    "print('Preprocessing the text...')\n",
    "tqdm.pandas(desc=\"Preprocessing the text\")\n",
    "data['text'] = data['text'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN...\n",
      "tweetid      0\n",
      "sentiment    0\n",
      "evidence     0\n",
      "text         0\n",
      "dtype: int64\n",
      "Calculating the average length of the text...\n",
      "4.892280404102722\n"
     ]
    }
   ],
   "source": [
    "# print nan\n",
    "print('Checking for NaN...')\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# print the average length of the text\n",
    "print('Calculating the average length of the text...')\n",
    "print(data['text'].apply(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "data['tokens'] = data['text'].apply(word_tokenize)\n",
    "\n",
    "# 創建詞彙表\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "counter = Counter(all_tokens)\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items(), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe model: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe model: 1193514it [01:31, 13045.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load pretrained GloVe model\n",
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading GloVe model...\")\n",
    "    model = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe model\"):\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float32)\n",
    "            model[word] = embedding\n",
    "    print(\"GloVe model loaded!\")\n",
    "    return model\n",
    "\n",
    "glove_model = load_glove_model('../dataset/glove_twitter_27B/glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))  # +1 for padding token\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Randomly initialize\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = [[vocab.get(token, 0) for token in tokens] for tokens in data['tokens']]\n",
    "max_len = 100\n",
    "padded_sequences = np.zeros((len(sequences), max_len), dtype=int)\n",
    "for i, seq in enumerate(sequences):\n",
    "    if len(seq) > 0:\n",
    "        padded_sequences[i, :len(seq)] = np.array(seq)[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "np.save('padded_sequences.npy', padded_sequences)\n",
    "np.save('labels.npy', data['sentiment'].values)\n",
    "np.save('embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
